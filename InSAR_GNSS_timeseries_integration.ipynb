{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bcda7ce0-920b-4d4c-8841-82a648a0f56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import subprocess\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from mintpy import view, plot_network\n",
    "from mintpy.objects import gnss, timeseries\n",
    "from mintpy.smallbaselineApp import TimeSeriesAnalysis\n",
    "from mintpy.utils import ptime, readfile, writefile, time_func, utils as ut, utils0 as ut0\n",
    "from scipy import signal\n",
    "from scipy.interpolate import Rbf\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8db40faf-7429-4583-baed-0a256ed91bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRE_GNSS_filename = r'C:\\Users\\bvarugu\\Documents\\ArcGIS\\Projects\\InSAR_discussion\\Outputs\\GNSS_station_network_TRE_Towill.csv'\n",
    "TRE_GNSS = pd.read_csv(TRE_GNSS_filename,header=0);\n",
    "TRE_GNSS['site'] = TRE_GNSS['CODE'].str[-4:];\n",
    "used_sites= list(TRE_GNSS['site'][TRE_GNSS['USED']==1]);\n",
    "filter_by_TRE = True;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6befe19c-b617-4a84-9c47-902014d00b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference date not set to first date. Setting it ....\n",
      "input reference date: 20211106\n",
      "--------------------------------------------------\n",
      "change reference date for file: C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\timeseries_tropHgt_demErr.h5\n",
      "split along y dimension (2188) into 2 boxes\n",
      "    with each box up to 1094 in y dimension\n",
      "\n",
      "------- processing patch 1 out of 2 --------------\n",
      "box width:  4017\n",
      "box length: 1094\n",
      "reading data ...\n",
      "referencing in time ...\n",
      "--------------------------------------------------\n",
      "open  HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\timeseries_tropHgt_demErr.h5 in r+ mode\n",
      "writing dataset /timeseries                block: (0, 82, 0, 1094, 0, 4017)\n",
      "close HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\timeseries_tropHgt_demErr.h5.\n",
      "\n",
      "------- processing patch 2 out of 2 --------------\n",
      "box width:  4017\n",
      "box length: 1094\n",
      "reading data ...\n",
      "referencing in time ...\n",
      "--------------------------------------------------\n",
      "open  HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\timeseries_tropHgt_demErr.h5 in r+ mode\n",
      "writing dataset /timeseries                block: (0, 82, 1094, 2188, 0, 4017)\n",
      "close HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\timeseries_tropHgt_demErr.h5.\n",
      "update \"REF_DATE\" attribute value to 20211106\n",
      "time used: 00 mins 6.7 secs.\n"
     ]
    }
   ],
   "source": [
    "InSAR_ts_filename =  r'C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\timeseries_tropHgt_demErr.h5';\n",
    "maskfile = os.path.join(os.path.dirname(InSAR_ts_filename),'maskTempCoh.h5');\n",
    "geometry_filename = os.path.join(os.path.dirname(InSAR_ts_filename),'geometryGeo.h5');\n",
    "azimuthAngle = readfile.read(geometry_filename,datasetName='azimuthAngle')[0];\n",
    "incidenceAngle = readfile.read(geometry_filename,datasetName='incidenceAngle')[0];\n",
    "save_gdal_filename = os.path.join(os.path.dirname(InSAR_ts_filename),'ARIASenA35_39N_Asc_velocity_2022_2024_GNSS.tif');\n",
    "maskTempCoh = readfile.read(maskfile)[0]\n",
    "gps_dir = os.path.join(r'C:\\Users\\bvarugu\\Documents\\GNSS','GNSS_ESESES');\n",
    "# get analysis metadata from InSAR velocity file\n",
    "insar_metadata = readfile.read_attribute(InSAR_ts_filename);\n",
    "date_list = timeseries(InSAR_ts_filename).get_date_list()\n",
    "if insar_metadata['REF_DATE'] != date_list[0]:\n",
    "    print('Reference date not set to first date. Setting it ....');\n",
    "    iargs = [InSAR_ts_filename, '--ref-date', str(date_list[0])]\n",
    "    import mintpy.cli.reference_date\n",
    "    mintpy.cli.reference_date.main(iargs)\n",
    "insar_ts, insar_metadata = readfile.read(InSAR_ts_filename);\n",
    "lat_step = float(insar_metadata['Y_STEP'])\n",
    "lon_step = float(insar_metadata['X_STEP'])\n",
    "(S,N,W,E) = ut.four_corners(insar_metadata)\n",
    "start_date = insar_metadata.get('START_DATE', None)\n",
    "end_date = insar_metadata.get('END_DATE', None)\n",
    "start_date_gnss = dt.datetime.strptime(start_date, \"%Y%m%d\")\n",
    "end_date_gnss = dt.datetime.strptime(end_date, \"%Y%m%d\")\n",
    "insar_ts = readfile.read(InSAR_ts_filename, datasetName='timeseries')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fbcafe40-a4fa-4dc5-b168-1fa495aab33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 3655 GNSS sites with fields: site lon lat\n",
      "keep sites within SNWE of (38.34165133, 40.164983934, -124.415783567, -121.068284906): [66]\n",
      "['CHO1' 'CHO5' 'DIXN' 'GASB' 'HOPB' 'LNC1' 'LNC2' 'MNRC' 'ORVB' 'P059'\n",
      " 'P156' 'P164' 'P182' 'P184' 'P185' 'P186' 'P187' 'P188' 'P189' 'P190'\n",
      " 'P192' 'P195' 'P197' 'P201' 'P202' 'P203' 'P204' 'P205' 'P206' 'P207'\n",
      " 'P208' 'P263' 'P264' 'P265' 'P267' 'P268' 'P269' 'P270' 'P271' 'P272'\n",
      " 'P276' 'P312' 'P313' 'P314' 'P315' 'P317' 'P318' 'P319' 'P320' 'P321'\n",
      " 'P322' 'P329' 'P333' 'P334' 'P335' 'P336' 'P339' 'P340' 'P344' 'P794'\n",
      " 'PLMO' 'PLSB' 'SUTB' 'TMB2' 'UCD1' 'VCVL']\n",
      "Initial list of 33 stations used in analysis:\n",
      "['DIXN', 'HOPB', 'MNRC', 'ORVB', 'P059', 'P182', 'P184', 'P185', 'P189', 'P190', 'P192', 'P197', 'P201', 'P202', 'P203', 'P204', 'P205', 'P206', 'P207', 'P208', 'P264', 'P265', 'P268', 'P271', 'P272', 'P276', 'P322', 'P335', 'P336', 'P339', 'P344', 'UCD1', 'VCVL']\n"
     ]
    }
   ],
   "source": [
    "gnss_source = 'ESESES';\n",
    "\n",
    "if filter_by_TRE==True:\n",
    "    gnss_names,gnss_lats, gnss_lons = gnss.search_gnss(SNWE=(S,N,W,E), source=gnss_source,start_date=start_date, end_date=end_date);\n",
    "    in_TRE = [i for i,gnss in enumerate(gnss_names) if gnss in used_sites];\n",
    "    site_names, site_lats, site_lons = gnss_names[in_TRE],gnss_lats[in_TRE],gnss_lons[in_TRE]\n",
    "else:\n",
    "    site_names, site_lats, site_lons = gnss.search_gnss(SNWE=(S,N,W,E), source=gnss_source,start_date=start_date, end_date=end_date);\n",
    "\n",
    "site_names = [str(stn) for stn in site_names]\n",
    "print(\"Initial list of {} stations used in analysis:\".format(len(site_names)))\n",
    "print(site_names)\n",
    "\n",
    "\n",
    "site_Y,site_X = ut.coordinate(insar_metadata).geo2radar(site_lats, site_lons)[0:2];\n",
    "site_azimuthAngles = [azimuthAngle[row,col] for row, col in zip(site_Y,site_X)];\n",
    "site_incidenceAngles = [incidenceAngle[row,col] for row, col in zip(site_Y,site_X)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e1747da-c72a-4f30-ae41-6a0dc87f83a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_longitude(lon, limit='-180to180'):\n",
    "    \"\"\"Normalize the longitude value range into (-180, 180] or [0, 360).\n",
    "\n",
    "    Parameters: lon   - float / np.ndarray, longitude in degree\n",
    "                limit - str, -180to180 or 0to360\n",
    "    Returns:    lon   - float / np.ndarray, longitude in degree\n",
    "    \"\"\"\n",
    "    lon = np.asarray(lon)\n",
    "\n",
    "    # ensure data within (-180, 360)\n",
    "    lon = np.where(lon >= 360, lon - 360, lon)\n",
    "    lon = np.where(lon <= -180, lon + 360, lon)\n",
    "\n",
    "    # range option 1: ensure data within (-180, 180]\n",
    "    if limit == '-180to180':\n",
    "        lon = np.where(lon > 180, lon - 360, lon)\n",
    "\n",
    "    # range option 2: ensure data within [0, 360)\n",
    "    elif limit == '0to360' and np.nanmin(lon) < 0:\n",
    "        lon = np.where(lon < 0, lon + 360, lon)\n",
    "\n",
    "    return float(lon) if np.isscalar(lon) else lon\n",
    "    \n",
    "class GNSS_ESESES:\n",
    "    \"\"\"GNSS child class for daily solutions processed for the Enhanced Solid\n",
    "    Earth Science ESDR System (ESESES) project by JPL and SOPAC.\n",
    "\n",
    "    Website: https://cddis.nasa.gov/Data_and_Derived_Products/GNSS/ESESES_products.html\n",
    "             http://garner.ucsd.edu/pub/measuresESESES_products/\n",
    "    \"\"\"\n",
    "    def __init__(self, site: str, data_dir=None, version='IGS14', url_prefix=None):\n",
    "        self.site = site\n",
    "        self.data_dir = data_dir or os.getcwd()  # Default to current working directory\n",
    "        self.version = version\n",
    "        self.source = 'ESESES'\n",
    "        self.url_prefix = url_prefix\n",
    "        \n",
    "\n",
    "        # get file\n",
    "        self.file = os.path.join(self.data_dir, f'{self.site.lower():s}CleanTrend.neu.Z')\n",
    "\n",
    "        # get url\n",
    "        # moved to GNSS_ESESES.dload_site() to avoid searching url_prefix\n",
    "        # when downloading is not needed.\n",
    "\n",
    "\n",
    "    def dload_site(self, overwrite=True, total_tries=5, print_msg=True):\n",
    "        \"\"\"Download GNSS data file.\n",
    "        \"\"\"\n",
    "        from zipfile import ZipFile\n",
    "\n",
    "        # get url\n",
    "        if not self.url_prefix:\n",
    "            self.url_prefix = gnss.get_ESESES_url_prefix()\n",
    "        self.url = os.path.join(self.url_prefix, os.path.basename(self.file)).replace(\"\\\\\", \"/\");#print('URL is',self.url);\n",
    "        if self.url and overwrite or not os.path.isfile(self.file):\n",
    "            print(f\"downloading site {self.site:s} from {self.source} to {self.file:s}\")\n",
    "            # retry on download fail\n",
    "            # https://stackoverflow.com/questions/31529151\n",
    "            remain_tries = total_tries\n",
    "            while remain_tries > 0 :\n",
    "                try:\n",
    "                    urlretrieve(self.url, self.file)\n",
    "                    print(f'successfully downloaded: {self.url}')\n",
    "                except:\n",
    "                    print(f'error downloading {self.url} on trial no. {total_tries-remain_tries}')\n",
    "                    remain_tries -= 1\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        # call parent class to download\n",
    "        #super().dload_site(overwrite=overwrite, print_msg=print_msg)\n",
    "\n",
    "        # uncompress the downloaded *.z file [for ESESES only]\n",
    "        with ZipFile(self.file, 'r') as fz:\n",
    "            fz.extractall(self.data_dir)\n",
    "        self.file = self.file.strip('.Z')    # update file name\n",
    "        if print_msg:\n",
    "            print(f'... extracted to {self.file:s}')\n",
    "\n",
    "        return self.file\n",
    "\n",
    "\n",
    "    def get_site_lat_lon(self, print_msg=False) -> (float, float):\n",
    "        \"\"\"Get station lat/lon based on processing source.\n",
    "        Retrieve data from the displacement file.\n",
    "\n",
    "        Modifies:   self.lat/lon - float\n",
    "        Returns:    self.lat/lon - float\n",
    "        \"\"\"\n",
    "        # download file if it does not exist\n",
    "        if not os.path.isfile(self.file):\n",
    "            self.dload_site(print_msg=print_msg)\n",
    "\n",
    "        # use the uncompressed data file\n",
    "        if self.file.endswith('.Z'):\n",
    "            self.file = self.file[:-2]\n",
    "\n",
    "        with open(self.file) as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "            # latitude\n",
    "            lat_line = [x for x in lines if x.startswith('# Latitude')][0].strip('\\n')\n",
    "            self.site_lat = float(lat_line.split()[-1])\n",
    "\n",
    "            # longitude\n",
    "            lon_line = [x for x in lines if x.startswith('# East Longitude')][0].strip('\\n')\n",
    "            self.site_lon = float(lon_line.split()[-1])\n",
    "\n",
    "        # ensure longitude in the range of (-180, 180]\n",
    "        self.site_lon = standardize_longitude(self.site_lon, limit='-180to180')\n",
    "\n",
    "        return self.site_lat, self.site_lon\n",
    "    def _crop_to_date_range(self, start_date: str, end_date: str):\n",
    "        \"\"\"Crop the time-series given the start/end_date in format YYYYMMDD,\n",
    "        and create date_list from dates.\n",
    "        \"\"\"\n",
    "        flag = np.ones(len(self.dates), dtype=bool)\n",
    "        if start_date:\n",
    "            t0 = ptime.date_list2vector([start_date])[0][0]\n",
    "            flag[self.dates < t0] = 0\n",
    "        if end_date:\n",
    "            t1 = ptime.date_list2vector([end_date])[0][0]\n",
    "            flag[self.dates > t1] = 0\n",
    "\n",
    "        self.dates = self.dates[flag]\n",
    "        self.dis_e = self.dis_e[flag]\n",
    "        self.dis_n = self.dis_n[flag]\n",
    "        self.dis_u = self.dis_u[flag]\n",
    "        self.std_e = self.std_e[flag]\n",
    "        self.std_n = self.std_n[flag]\n",
    "        self.std_u = self.std_u[flag]\n",
    "\n",
    "        # create member var: date_list\n",
    "        self.date_list = [x.strftime('%Y%m%d') for x in self.dates]\n",
    "\n",
    "\n",
    "    def read_displacement(self, start_date=None, end_date=None, print_msg=True, display=False):\n",
    "        \"\"\"Read GNSS displacement time-series (defined by start/end_date).\n",
    "\n",
    "        Parameters: start/end_date - str, date in YYYYMMDD format\n",
    "        Returns:    dates          - 1D np.ndarray of datetime.datetime object\n",
    "                    dis_e/n/u      - 1D np.ndarray of displacement in meters in float32\n",
    "                    std_e/n/u      - 1D np.ndarray of displacement STD in meters in float32\n",
    "        \"\"\"\n",
    "        vprint = print if print_msg else lambda *args, **kwargs: None\n",
    "\n",
    "        # download file if it does not exist\n",
    "        if not os.path.isfile(self.file):\n",
    "            self.dload_site(print_msg=print_msg)\n",
    "\n",
    "        # use the uncompressed data file\n",
    "        if self.file.endswith('.Z'):\n",
    "            self.file = self.file[:-2]\n",
    "\n",
    "        # read data file\n",
    "        # use the first 9 cols only, as some epoches miss 10-13 cols: CorrNE/NU/EU, Chi-Squared\n",
    "        vprint('reading time and displacement in east/north/vertical direction')\n",
    "        fc = np.loadtxt(self.file, usecols=tuple(range(0,9)))\n",
    "        num_solution = fc.shape[0]\n",
    "\n",
    "        # parse dates\n",
    "        dates = [dt.datetime(int(fc[i, 1]), 1, 1) + dt.timedelta(days=int(fc[i, 2]))\n",
    "                 for i in range(num_solution)]\n",
    "        self.dates = np.array(dates)\n",
    "\n",
    "        # parse displacement data\n",
    "        (self.dis_n,\n",
    "         self.dis_e,\n",
    "         self.dis_u,\n",
    "         self.std_n,\n",
    "         self.std_e,\n",
    "         self.std_u) = fc[:, 3:9].astype(np.float32).T / 1000\n",
    "\n",
    "        # cut out the specified time range\n",
    "        self._crop_to_date_range(start_date, end_date)\n",
    "\n",
    "        # display if requested\n",
    "        if display:\n",
    "            self.plot()\n",
    "\n",
    "        return (self.dates,\n",
    "                self.dis_e, self.dis_n, self.dis_u,\n",
    "                self.std_e, self.std_n, self.std_u)\n",
    "\n",
    "\n",
    "\n",
    "gps_plot_dir = os.path.join(gps_dir,'GNSS_plot');\n",
    "os.makedirs(gps_plot_dir,exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "270ce6b2-33ac-41ec-a5b1-2e94d104e4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading time and displacement in east/north/vertical direction\n",
      "reading time and displacement in east/north/vertical direction\n",
      "reading time and displacement in east/north/vertical direction\n",
      "searching for ESESES url_prefix ...\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241224 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241223 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241222 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241221 [YES!]\n",
      "downloading site ORVB from ESESES to C:\\Users\\bvarugu\\Documents\\GNSS\\GNSS_ESESES\\orvbCleanTrend.neu.Z\n",
      "successfully downloaded: http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241221/orvbCleanTrend.neu.Z\n",
      "... extracted to C:\\Users\\bvarugu\\Documents\\GNSS\\GNSS_ESESES\\orvbCleanTrend.neu\n",
      "reading time and displacement in east/north/vertical direction\n",
      "reading time and displacement in east/north/vertical direction\n",
      "reading time and displacement in east/north/vertical direction\n",
      "searching for ESESES url_prefix ...\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241224 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241223 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241222 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241221 [YES!]\n",
      "downloading site P184 from ESESES to C:\\Users\\bvarugu\\Documents\\GNSS\\GNSS_ESESES\\p184CleanTrend.neu.Z\n",
      "successfully downloaded: http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241221/p184CleanTrend.neu.Z\n",
      "... extracted to C:\\Users\\bvarugu\\Documents\\GNSS\\GNSS_ESESES\\p184CleanTrend.neu\n",
      "reading time and displacement in east/north/vertical direction\n",
      "searching for ESESES url_prefix ...\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241224 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241223 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241222 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241221 [YES!]\n",
      "downloading site P185 from ESESES to C:\\Users\\bvarugu\\Documents\\GNSS\\GNSS_ESESES\\p185CleanTrend.neu.Z\n",
      "successfully downloaded: http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241221/p185CleanTrend.neu.Z\n",
      "... extracted to C:\\Users\\bvarugu\\Documents\\GNSS\\GNSS_ESESES\\p185CleanTrend.neu\n",
      "reading time and displacement in east/north/vertical direction\n",
      "reading time and displacement in east/north/vertical direction\n",
      "searching for ESESES url_prefix ...\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241224 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241223 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241222 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241221 [YES!]\n",
      "downloading site P190 from ESESES to C:\\Users\\bvarugu\\Documents\\GNSS\\GNSS_ESESES\\p190CleanTrend.neu.Z\n",
      "successfully downloaded: http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241221/p190CleanTrend.neu.Z\n",
      "... extracted to C:\\Users\\bvarugu\\Documents\\GNSS\\GNSS_ESESES\\p190CleanTrend.neu\n",
      "reading time and displacement in east/north/vertical direction\n",
      "searching for ESESES url_prefix ...\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241224 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241223 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241222 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241221 [YES!]\n",
      "downloading site P192 from ESESES to C:\\Users\\bvarugu\\Documents\\GNSS\\GNSS_ESESES\\p192CleanTrend.neu.Z\n",
      "successfully downloaded: http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241221/p192CleanTrend.neu.Z\n",
      "... extracted to C:\\Users\\bvarugu\\Documents\\GNSS\\GNSS_ESESES\\p192CleanTrend.neu\n",
      "reading time and displacement in east/north/vertical direction\n",
      "reading time and displacement in east/north/vertical direction\n",
      "reading time and displacement in east/north/vertical direction\n",
      "reading time and displacement in east/north/vertical direction\n",
      "reading time and displacement in east/north/vertical direction\n",
      "reading time and displacement in east/north/vertical direction\n",
      "searching for ESESES url_prefix ...\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241224 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241223 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241222 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241221 [YES!]\n",
      "downloading site P205 from ESESES to C:\\Users\\bvarugu\\Documents\\GNSS\\GNSS_ESESES\\p205CleanTrend.neu.Z\n",
      "successfully downloaded: http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241221/p205CleanTrend.neu.Z\n",
      "... extracted to C:\\Users\\bvarugu\\Documents\\GNSS\\GNSS_ESESES\\p205CleanTrend.neu\n",
      "reading time and displacement in east/north/vertical direction\n",
      "reading time and displacement in east/north/vertical direction\n",
      "searching for ESESES url_prefix ...\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241224 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241223 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241222 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241221 [YES!]\n",
      "downloading site P207 from ESESES to C:\\Users\\bvarugu\\Documents\\GNSS\\GNSS_ESESES\\p207CleanTrend.neu.Z\n",
      "successfully downloaded: http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241221/p207CleanTrend.neu.Z\n",
      "... extracted to C:\\Users\\bvarugu\\Documents\\GNSS\\GNSS_ESESES\\p207CleanTrend.neu\n",
      "reading time and displacement in east/north/vertical direction\n",
      "searching for ESESES url_prefix ...\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241224 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241223 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241222 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241221 [YES!]\n",
      "downloading site P208 from ESESES to C:\\Users\\bvarugu\\Documents\\GNSS\\GNSS_ESESES\\p208CleanTrend.neu.Z\n",
      "successfully downloaded: http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241221/p208CleanTrend.neu.Z\n",
      "... extracted to C:\\Users\\bvarugu\\Documents\\GNSS\\GNSS_ESESES\\p208CleanTrend.neu\n",
      "reading time and displacement in east/north/vertical direction\n",
      "reading time and displacement in east/north/vertical direction\n",
      "reading time and displacement in east/north/vertical direction\n",
      "reading time and displacement in east/north/vertical direction\n",
      "reading time and displacement in east/north/vertical direction\n",
      "searching for ESESES url_prefix ...\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241224 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241223 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241222 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241221 [YES!]\n",
      "downloading site P272 from ESESES to C:\\Users\\bvarugu\\Documents\\GNSS\\GNSS_ESESES\\p272CleanTrend.neu.Z\n",
      "successfully downloaded: http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241221/p272CleanTrend.neu.Z\n",
      "... extracted to C:\\Users\\bvarugu\\Documents\\GNSS\\GNSS_ESESES\\p272CleanTrend.neu\n",
      "reading time and displacement in east/north/vertical direction\n",
      "reading time and displacement in east/north/vertical direction\n",
      "searching for ESESES url_prefix ...\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241224 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241223 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241222 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241221 [YES!]\n",
      "downloading site P322 from ESESES to C:\\Users\\bvarugu\\Documents\\GNSS\\GNSS_ESESES\\p322CleanTrend.neu.Z\n",
      "successfully downloaded: http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241221/p322CleanTrend.neu.Z\n",
      "... extracted to C:\\Users\\bvarugu\\Documents\\GNSS\\GNSS_ESESES\\p322CleanTrend.neu\n",
      "reading time and displacement in east/north/vertical direction\n",
      "searching for ESESES url_prefix ...\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241224 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241223 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241222 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241221 [YES!]\n",
      "downloading site P335 from ESESES to C:\\Users\\bvarugu\\Documents\\GNSS\\GNSS_ESESES\\p335CleanTrend.neu.Z\n",
      "successfully downloaded: http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241221/p335CleanTrend.neu.Z\n",
      "... extracted to C:\\Users\\bvarugu\\Documents\\GNSS\\GNSS_ESESES\\p335CleanTrend.neu\n",
      "reading time and displacement in east/north/vertical direction\n",
      "searching for ESESES url_prefix ...\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241224 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241223 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241222 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241221 [YES!]\n",
      "downloading site P336 from ESESES to C:\\Users\\bvarugu\\Documents\\GNSS\\GNSS_ESESES\\p336CleanTrend.neu.Z\n",
      "successfully downloaded: http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241221/p336CleanTrend.neu.Z\n",
      "... extracted to C:\\Users\\bvarugu\\Documents\\GNSS\\GNSS_ESESES\\p336CleanTrend.neu\n",
      "reading time and displacement in east/north/vertical direction\n",
      "searching for ESESES url_prefix ...\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241224 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241223 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241222 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241221 [YES!]\n",
      "downloading site P339 from ESESES to C:\\Users\\bvarugu\\Documents\\GNSS\\GNSS_ESESES\\p339CleanTrend.neu.Z\n",
      "successfully downloaded: http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241221/p339CleanTrend.neu.Z\n",
      "... extracted to C:\\Users\\bvarugu\\Documents\\GNSS\\GNSS_ESESES\\p339CleanTrend.neu\n",
      "reading time and displacement in east/north/vertical direction\n",
      "searching for ESESES url_prefix ...\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241224 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241223 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241222 [no]\n",
      "http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241221 [YES!]\n",
      "downloading site P344 from ESESES to C:\\Users\\bvarugu\\Documents\\GNSS\\GNSS_ESESES\\p344CleanTrend.neu.Z\n",
      "successfully downloaded: http://garner.ucsd.edu/pub/measuresESESES_products/Timeseries/CurrentUntarred/Clean_TrendNeuTimeSeries_comb_20241221/p344CleanTrend.neu.Z\n",
      "... extracted to C:\\Users\\bvarugu\\Documents\\GNSS\\GNSS_ESESES\\p344CleanTrend.neu\n",
      "reading time and displacement in east/north/vertical direction\n",
      "reading time and displacement in east/north/vertical direction\n",
      "reading time and displacement in east/north/vertical direction\n"
     ]
    }
   ],
   "source": [
    "wind=2;  #Window for InSAR velocity\n",
    "# # Get date list\n",
    "\n",
    "num_date = len(date_list)\n",
    "date0, date1 = date_list[0], date_list[-1]\n",
    "insar_dates = ptime.date_list2vector(date_list)[0]\n",
    "\n",
    "\n",
    "site_displacements = np.zeros([len(site_names),len(insar_dates)]);\n",
    "site_insar_displacements = np.zeros([len(site_names),len(insar_dates)]);\n",
    "for i, site_name in enumerate(site_names):\n",
    "    gnss_obj = GNSS_ESESES(site =site_name,data_dir=gps_dir);gnss_obj.read_displacement();\n",
    "    #gnss_obj._crop_to_date_range(start_date= date0, end_date= date1);\n",
    "    gnss_obj_dis_los = ut.enu2los(gnss_obj.dis_e,gnss_obj.dis_n, gnss_obj.dis_u, site_incidenceAngles[i], site_azimuthAngles[i]);\n",
    "    y, x = int(site_Y[i]),int(site_X[i]);\n",
    "    for j in range(len(insar_dates)):\n",
    "        insar_disp = insar_ts[j];\n",
    "        # desired_index = np.where((gnss_obj.dates > date - dt.timedelta(days=3)) &\n",
    "        #                      (gnss_obj.dates < date + dt.timedelta(days=3)))[0]\n",
    "        # site_displacements[i,j]= np.nanmean(gnss_obj_dis_los[desired_index]);\n",
    "        desired_index = np.where(gnss_obj.dates == insar_dates[j])[0];\n",
    "        try:\n",
    "            site_displacements[i,j]= gnss_obj_dis_los[desired_index][0];\n",
    "        except:\n",
    "            site_displacements[i,j]= np.nan;\n",
    "        site_insar_displacements[i,j] = np.nanmean(insar_disp[y-wind:y+wind, x-wind:x+wind],axis=(0,1));\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "190bfc34-95e5-46f9-a432-c47c7c109c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(insar_dates)):\n",
    "    if np.isnan(site_displacements[:,i]).all() == True:\n",
    "        print('replacing values for {} from {}'.format(insar_dates[i],insar_dates[i-1]))\n",
    "        site_displacements[:,i] = site_displacements[:,i-1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05970b2e-ce8c-46e5-92dc-d10db470a7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_surface(station_names,station_Y,station_X,station_velocities,station_insar_velocites,insar_velocity):\n",
    "    station_names = np.array(station_names);\n",
    "    station_Y = np.array(station_Y);\n",
    "    station_X = np.array(station_X);\n",
    "    median_velocity = np.nanmedian(station_velocities)\n",
    "    print('Remove NaN Values and velcoties grater than median velocity:{}'.format(median_velocity))\n",
    "    valid_points = (~np.isnan(station_velocities)) & (~np.isnan(station_insar_velocites))\n",
    "    station_X_valid = station_X[valid_points]\n",
    "    station_Y_valid = station_Y[valid_points]\n",
    "    station_velocities_valid = station_velocities[valid_points];\n",
    "    station_insar_velocities_valid = station_insar_velocites[valid_points];\n",
    "    station_names_valid = station_names[valid_points];\n",
    "    \n",
    "    print('Remove Duplicates')\n",
    "    # Combine station_X_valid and station_Y_valid into a single array of coordinates\n",
    "    coords = np.array(list(zip(station_X_valid, station_Y_valid)))\n",
    "    \n",
    "    # Find unique coordinates and their indices\n",
    "    _, unique_indices = np.unique(coords, axis=0, return_index=True)\n",
    "    \n",
    "    # Use these indices to filter station_X_valid, station_Y_valid, station_velocities_valid, and station_names_valid\n",
    "    station_X_unique = station_X_valid[unique_indices]\n",
    "    station_Y_unique = station_Y_valid[unique_indices]\n",
    "    station_velocities_unique = station_velocities_valid[unique_indices]\n",
    "    station_insar_velocities_unique = station_insar_velocities_valid[unique_indices]\n",
    "    station_names_unique = station_names_valid[unique_indices]\n",
    "    station_residual_velocities_unique = station_velocities_unique-station_insar_velocities_unique\n",
    "    rbf_interpolator = Rbf(station_X_unique, station_Y_unique, station_residual_velocities_unique, function='linear')\n",
    "    rbf_interpolator2 = Rbf(station_X_unique, station_Y_unique, station_velocities_unique, function='linear')\n",
    "    # Define grid for interpolation (extend beyond the data range)\n",
    "    length, width = np.shape(insar_velocity)\n",
    "    grid_Y, grid_X = np.arange(0, length), np.arange(0,width)\n",
    "    grid_X, grid_Y = np.meshgrid(grid_X, grid_Y)\n",
    "    \n",
    "    # Interpolate on the extended grid\n",
    "    z_grid_extended = rbf_interpolator(grid_X, grid_Y)\n",
    "    gnss_vel_grid = rbf_interpolator2(grid_X, grid_Y);\n",
    "    print('Done')\n",
    "    return z_grid_extended,gnss_vel_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca769a24-8111-4793-b122-1b4f3b61ced3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-18 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:0.0016409270465373993\n",
      "Remove Duplicates\n",
      "Done\n",
      "2021-11-30 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:0.0031917765736579895\n",
      "Remove Duplicates\n",
      "Done\n",
      "2021-12-12 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:0.004198230803012848\n",
      "Remove Duplicates\n",
      "Done\n",
      "2022-04-05 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:0.008572164922952652\n",
      "Remove Duplicates\n",
      "Done\n",
      "2022-04-17 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:0.002594888210296631\n",
      "Remove Duplicates\n",
      "Done\n",
      "2022-04-29 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:0.001648621866479516\n",
      "Remove Duplicates\n",
      "Done\n",
      "2022-05-11 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.0016478421166539192\n",
      "Remove Duplicates\n",
      "Done\n",
      "2022-05-23 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:0.0024733515456318855\n",
      "Remove Duplicates\n",
      "Done\n",
      "2022-06-04 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:0.005011396482586861\n",
      "Remove Duplicates\n",
      "Done\n",
      "2022-06-16 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:0.002448759973049164\n",
      "Remove Duplicates\n",
      "Done\n",
      "2022-06-28 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.002454022876918316\n",
      "Remove Duplicates\n",
      "Done\n",
      "2022-07-22 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:0.00017085671424865723\n",
      "Remove Duplicates\n",
      "Done\n",
      "2022-08-03 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.0015965886414051056\n",
      "Remove Duplicates\n",
      "Done\n",
      "2022-08-15 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.0021614432334899902\n",
      "Remove Duplicates\n",
      "Done\n",
      "2022-08-27 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.0005324482917785645\n",
      "Remove Duplicates\n",
      "Done\n",
      "2022-09-08 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:0.0031186409760266542\n",
      "Remove Duplicates\n",
      "Done\n",
      "2022-09-20 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:0.001317877322435379\n",
      "Remove Duplicates\n",
      "Done\n",
      "2022-10-02 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:0.0010357368737459183\n",
      "Remove Duplicates\n",
      "Done\n",
      "2022-10-14 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:0.004165664315223694\n",
      "Remove Duplicates\n",
      "Done\n",
      "2022-10-26 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.002592356875538826\n",
      "Remove Duplicates\n",
      "Done\n",
      "2022-11-07 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:0.001403103582561016\n",
      "Remove Duplicates\n",
      "Done\n",
      "2022-11-19 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-4.73707914352417e-05\n",
      "Remove Duplicates\n",
      "Done\n",
      "2022-12-01 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:0.003900391049683094\n",
      "Remove Duplicates\n",
      "Done\n",
      "2022-12-13 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.001395437866449356\n",
      "Remove Duplicates\n",
      "Done\n",
      "2022-12-25 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:0.001072075217962265\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-01-06 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:0.00038989982567727566\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-01-18 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.0016023367643356323\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-01-30 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.00482587143778801\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-02-11 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.005272549809888005\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-02-23 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.004777162801474333\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-03-07 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.008217092603445053\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-03-19 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.00652051717042923\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-03-31 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.007075170055031776\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-04-12 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.003990627825260162\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-04-24 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.000643337145447731\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-05-06 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.004409072455018759\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-05-18 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.00514974445104599\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-05-30 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.00430098082870245\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-06-11 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.003943188115954399\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-06-23 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.007925207260996103\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-07-05 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.004888329654932022\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-07-17 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.005548157729208469\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-07-29 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.005663719028234482\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-08-10 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.0036061983555555344\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-08-22 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.005047038197517395\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-09-03 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.0072490896563977\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-09-15 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.0027931262739002705\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-09-27 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.0031742609571665525\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-10-09 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:0.00044501200318336487\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-10-21 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.0021542739123106003\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-11-02 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.005679577589035034\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-11-14 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.008640319108963013\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-11-26 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.008982280269265175\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-12-08 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.0018745176494121552\n",
      "Remove Duplicates\n",
      "Done\n",
      "2023-12-20 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:0.001079462468624115\n",
      "Remove Duplicates\n",
      "Done\n",
      "2024-01-01 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.002926517277956009\n",
      "Remove Duplicates\n",
      "Done\n",
      "2024-01-13 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.005390483653172851\n",
      "Remove Duplicates\n",
      "Done\n",
      "2024-01-25 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:0.002146429382264614\n",
      "Remove Duplicates\n",
      "Done\n",
      "2024-02-06 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.005100557813420892\n",
      "Remove Duplicates\n",
      "Done\n",
      "2024-02-18 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.004149987827986479\n",
      "Remove Duplicates\n",
      "Done\n",
      "2024-03-01 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.0041529834270477295\n",
      "Remove Duplicates\n",
      "Done\n",
      "2024-03-13 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.004225387703627348\n",
      "Remove Duplicates\n",
      "Done\n",
      "2024-03-25 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.007175877690315247\n",
      "Remove Duplicates\n",
      "Done\n",
      "2024-04-06 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.006731577217578888\n",
      "Remove Duplicates\n",
      "Done\n",
      "2024-04-18 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.006495371460914612\n",
      "Remove Duplicates\n",
      "Done\n",
      "2024-04-30 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.0046667177230119705\n",
      "Remove Duplicates\n",
      "Done\n",
      "2024-05-12 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.007155060768127441\n",
      "Remove Duplicates\n",
      "Done\n",
      "2024-05-24 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.004469062201678753\n",
      "Remove Duplicates\n",
      "Done\n",
      "2024-06-05 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.005443439586088061\n",
      "Remove Duplicates\n",
      "Done\n",
      "2024-06-29 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.013193614780902863\n",
      "Remove Duplicates\n",
      "Done\n",
      "2024-07-23 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.010462777456268668\n",
      "Remove Duplicates\n",
      "Done\n",
      "2024-08-04 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.010976594174280763\n",
      "Remove Duplicates\n",
      "Done\n",
      "2024-08-16 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.008918386185541749\n",
      "Remove Duplicates\n",
      "Done\n",
      "2024-08-28 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.011748472228646278\n",
      "Remove Duplicates\n",
      "Done\n",
      "2024-09-09 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.006908709183335304\n",
      "Remove Duplicates\n",
      "Done\n",
      "2024-09-21 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.006591524928808212\n",
      "Remove Duplicates\n",
      "Done\n",
      "2024-10-03 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.009446525946259499\n",
      "Remove Duplicates\n",
      "Done\n",
      "2024-10-15 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.009903224650770426\n",
      "Remove Duplicates\n",
      "Done\n",
      "2024-10-27 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.009548948146402836\n",
      "Remove Duplicates\n",
      "Done\n",
      "2024-11-08 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.009018700569868088\n",
      "Remove Duplicates\n",
      "Done\n",
      "2024-11-20 00:00:00\n",
      "Remove NaN Values and velcoties grater than median velocity:-0.011101913638412952\n",
      "Remove Duplicates\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "gnss_ts = np.zeros(np.shape(insar_ts), dtype=np.float32)\n",
    "residual_ts = np.zeros(np.shape(insar_ts), dtype=np.float32)\n",
    "site_displacements_referenced = site_displacements - site_displacements[:, [0]]\n",
    "for k,date in enumerate(insar_dates):\n",
    "    if k !=0:\n",
    "        print(date)\n",
    "        residual_ts[k,:,:],gnss_ts[k,:,:]  = get_surface(site_names,site_Y,site_X,site_displacements_referenced[:,k],site_insar_displacements[:,k],\n",
    "                                                        insar_ts[0]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a612773-1410-4107-8000-09a559b57a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_reference(cummulative_disp):\n",
    "    # Mask invalid values as NaN\n",
    "    cummulative_disp[maskTempCoh == 0] = np.nan\n",
    "    \n",
    "    # Find the minimum absolute value ignoring NaNs\n",
    "    min_value = np.nanmin(abs(cummulative_disp))\n",
    "    \n",
    "    # Find the index of the minimum value\n",
    "    index = np.unravel_index(np.nanargmin(abs(cummulative_disp)), cummulative_disp.shape)\n",
    "    \n",
    "    print(\"Minimum Value:\", min_value)\n",
    "    print(\"Index of Minimum Value:\", index)\n",
    "    return index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c616c6bf-a3ef-4adf-ba76-95027388fe07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Value: 1.3969839e-09\n",
      "Index of Minimum Value: (148, 2233)\n",
      "New reference point is: 148 2233\n",
      "create HDF5 file: C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\corrected_ts_TRE_GNSS.h5 with w mode\n",
      "create dataset /date       of |S8        in size of (82,)                with compression=None\n",
      "create dataset /timeseries of float32    in size of (82, 2188, 4017)     with compression=None\n",
      "finished writing to C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\corrected_ts_TRE_GNSS.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\bvarugu\\\\Documents\\\\ARIASenA35\\\\track39W\\\\mintpy\\\\corrected_ts_TRE_GNSS.h5'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add residual to the InSAR timeseries\n",
    "corrected_ts = insar_ts + residual_ts;\n",
    "cummulative_disp = corrected_ts[-1].copy()\n",
    "(new_ref_y,new_ref_x) = get_new_reference(cummulative_disp);\n",
    "print('New reference point is:',new_ref_y,new_ref_x);\n",
    "insar_metadata['REF_Y'] = str(new_ref_y);\n",
    "insar_metadata['REF_X'] = str(new_ref_x);\n",
    "#define file names\n",
    "date_list = np.array(date_list, dtype=np.bytes_)\n",
    "residual_ts_outname = os.path.join(os.path.dirname(InSAR_ts_filename),'residual_ts.h5');\n",
    "gnss_ts_outname = os.path.join(os.path.dirname(InSAR_ts_filename),'gnss_ts.h5');\n",
    "corrected_ts_outname = os.path.join(os.path.dirname(InSAR_ts_filename),'corrected_ts_TRE_GNSS.h5');\n",
    "#make a dict to write mintpy timeseries file\n",
    "residual_ts_dict = {\n",
    "    \"date\"       : date_list,\n",
    "    \"timeseries\" : residual_ts,\n",
    "}\n",
    "gnss_ts_dict = {\n",
    "    \"date\"       : date_list,\n",
    "    \"timeseries\" : gnss_ts,\n",
    "}\n",
    "corrected_ts_dict = {\n",
    "    \"date\"       : date_list,\n",
    "    \"timeseries\" : corrected_ts,\n",
    "}\n",
    "#writefile.write(residual_ts_dict,residual_ts_outname,metadata=insar_metadata);\n",
    "#writefile.write(gnss_ts_dict,gnss_ts_outname,metadata=insar_metadata);\n",
    "writefile.write(corrected_ts_dict,corrected_ts_outname,metadata=insar_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30030ee0-a495-44c4-9216-d0afbf5c5de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update mode: ON\n",
      "1) output file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5 NOT found.\n",
      "run or skip: run.\n",
      "open timeseries file: corrected_ts_TRE_GNSS.h5\n",
      "--------------------------------------------------\n",
      "dates from input file: 82\n",
      "['20211106', '20211118', '20211130', '20211212', '20220405', '20220417', '20220429', '20220511', '20220523', '20220604', '20220616', '20220628', '20220722', '20220803', '20220815', '20220827', '20220908', '20220920', '20221002', '20221014', '20221026', '20221107', '20221119', '20221201', '20221213', '20221225', '20230106', '20230118', '20230130', '20230211', '20230223', '20230307', '20230319', '20230331', '20230412', '20230424', '20230506', '20230518', '20230530', '20230611', '20230623', '20230705', '20230717', '20230729', '20230810', '20230822', '20230903', '20230915', '20230927', '20231009', '20231021', '20231102', '20231114', '20231126', '20231208', '20231220', '20240101', '20240113', '20240125', '20240206', '20240218', '20240301', '20240313', '20240325', '20240406', '20240418', '20240430', '20240512', '20240524', '20240605', '20240629', '20240723', '20240804', '20240816', '20240828', '20240909', '20240921', '20241003', '20241015', '20241027', '20241108', '20241120']\n",
      "--------------------------------------------------\n",
      "using all dates to calculate the time function\n",
      "--------------------------------------------------\n",
      "estimate deformation model with the following assumed time functions:\n",
      "    polynomial : 1\n",
      "    periodic   : []\n",
      "    stepDate   : []\n",
      "    polyline   : []\n",
      "    exp        : {}\n",
      "    log        : {}\n",
      "add/update the following configuration metadata:\n",
      "['startDate', 'endDate', 'excludeDate', 'polynomial', 'periodic', 'stepDate', 'exp', 'log', 'uncertaintyQuantification', 'timeSeriesCovFile', 'bootstrapCount']\n",
      "--------------------------------------------------\n",
      "create HDF5 file: C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5 with w mode\n",
      "create dataset  : intercept    of <class 'numpy.float32'>   in size of (2188, 4017)         with compression = None\n",
      "create dataset  : interceptStd of <class 'numpy.float32'>   in size of (2188, 4017)         with compression = None\n",
      "create dataset  : velocity     of <class 'numpy.float32'>   in size of (2188, 4017)         with compression = None\n",
      "create dataset  : velocityStd  of <class 'numpy.float32'>   in size of (2188, 4017)         with compression = None\n",
      "create dataset  : residue      of <class 'numpy.float32'>   in size of (2188, 4017)         with compression = None\n",
      "add /intercept    attribute: UNIT = m\n",
      "add /interceptStd attribute: UNIT = m\n",
      "add /velocity     attribute: UNIT = m/year\n",
      "add /velocityStd  attribute: UNIT = m/year\n",
      "add /residue      attribute: UNIT = m\n",
      "close  HDF5 file: C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5\n",
      "split along y dimension (2188) into 3 boxes\n",
      "    with each box up to 730 in y dimension\n",
      "\n",
      "------- processing patch 1 out of 3 --------------\n",
      "box width:  4017\n",
      "box length: 730\n",
      "reading data from file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\corrected_ts_TRE_GNSS.h5 ...\n",
      "skip pixels with zero/nan value in all acquisitions\n",
      "number of pixels to invert: 2932410 out of 2932410 (100.0%)\n",
      "estimating time functions via linalg.lstsq ...\n",
      "estimating time functions STD from time-series fitting residual ...\n",
      "--------------------------------------------------\n",
      "open  HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5 in a mode\n",
      "writing dataset /intercept                 block: [0, 730, 0, 4017]\n",
      "close HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5.\n",
      "--------------------------------------------------\n",
      "open  HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5 in a mode\n",
      "writing dataset /interceptStd              block: [0, 730, 0, 4017]\n",
      "close HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5.\n",
      "--------------------------------------------------\n",
      "open  HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5 in a mode\n",
      "writing dataset /velocity                  block: [0, 730, 0, 4017]\n",
      "close HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5.\n",
      "--------------------------------------------------\n",
      "open  HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5 in a mode\n",
      "writing dataset /velocityStd               block: [0, 730, 0, 4017]\n",
      "close HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5.\n",
      "--------------------------------------------------\n",
      "open  HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5 in a mode\n",
      "writing dataset /residue                   block: [0, 730, 0, 4017]\n",
      "close HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5.\n",
      "\n",
      "------- processing patch 2 out of 3 --------------\n",
      "box width:  4017\n",
      "box length: 730\n",
      "reading data from file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\corrected_ts_TRE_GNSS.h5 ...\n",
      "skip pixels with zero/nan value in all acquisitions\n",
      "number of pixels to invert: 2932410 out of 2932410 (100.0%)\n",
      "estimating time functions via linalg.lstsq ...\n",
      "estimating time functions STD from time-series fitting residual ...\n",
      "--------------------------------------------------\n",
      "open  HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5 in a mode\n",
      "writing dataset /intercept                 block: [730, 1460, 0, 4017]\n",
      "close HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5.\n",
      "--------------------------------------------------\n",
      "open  HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5 in a mode\n",
      "writing dataset /interceptStd              block: [730, 1460, 0, 4017]\n",
      "close HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5.\n",
      "--------------------------------------------------\n",
      "open  HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5 in a mode\n",
      "writing dataset /velocity                  block: [730, 1460, 0, 4017]\n",
      "close HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5.\n",
      "--------------------------------------------------\n",
      "open  HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5 in a mode\n",
      "writing dataset /velocityStd               block: [730, 1460, 0, 4017]\n",
      "close HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5.\n",
      "--------------------------------------------------\n",
      "open  HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5 in a mode\n",
      "writing dataset /residue                   block: [730, 1460, 0, 4017]\n",
      "close HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5.\n",
      "\n",
      "------- processing patch 3 out of 3 --------------\n",
      "box width:  4017\n",
      "box length: 728\n",
      "reading data from file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\corrected_ts_TRE_GNSS.h5 ...\n",
      "skip pixels with zero/nan value in all acquisitions\n",
      "number of pixels to invert: 2924376 out of 2924376 (100.0%)\n",
      "estimating time functions via linalg.lstsq ...\n",
      "estimating time functions STD from time-series fitting residual ...\n",
      "--------------------------------------------------\n",
      "open  HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5 in a mode\n",
      "writing dataset /intercept                 block: [1460, 2188, 0, 4017]\n",
      "close HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5.\n",
      "--------------------------------------------------\n",
      "open  HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5 in a mode\n",
      "writing dataset /interceptStd              block: [1460, 2188, 0, 4017]\n",
      "close HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5.\n",
      "--------------------------------------------------\n",
      "open  HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5 in a mode\n",
      "writing dataset /velocity                  block: [1460, 2188, 0, 4017]\n",
      "close HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5.\n",
      "--------------------------------------------------\n",
      "open  HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5 in a mode\n",
      "writing dataset /velocityStd               block: [1460, 2188, 0, 4017]\n",
      "close HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5.\n",
      "--------------------------------------------------\n",
      "open  HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5 in a mode\n",
      "writing dataset /residue                   block: [1460, 2188, 0, 4017]\n",
      "close HDF5 file C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5.\n",
      "time used: 00 mins 9.3 secs.\n"
     ]
    }
   ],
   "source": [
    "corrected_vel_outname = os.path.join(os.path.dirname(InSAR_ts_filename),'velocity_GNSS_integrated.h5');\n",
    "iargs = [corrected_ts_outname, '-o', corrected_vel_outname, '--update']\n",
    "import mintpy.cli.timeseries2velocity\n",
    "mintpy.cli.timeseries2velocity.main(iargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d65ea2d-c2ab-4b02-8b3b-95feb1d14256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masking velocity     from C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5 ...\n",
      "masking velocityStd  from C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5 ...\n",
      "masking intercept    from C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5 ...\n",
      "masking interceptStd from C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5 ...\n",
      "masking residue      from C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated.h5 ...\n",
      "create HDF5 file: C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated_msk.h5 with w mode\n",
      "create dataset /velocity     of float32    in size of (2188, 4017)         with compression=None\n",
      "create dataset /velocityStd  of float32    in size of (2188, 4017)         with compression=None\n",
      "create dataset /intercept    of float32    in size of (2188, 4017)         with compression=None\n",
      "create dataset /interceptStd of float32    in size of (2188, 4017)         with compression=None\n",
      "create dataset /residue      of float32    in size of (2188, 4017)         with compression=None\n",
      "finished writing to C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\\velocity_GNSS_integrated_msk.h5\n"
     ]
    }
   ],
   "source": [
    "corrected_vel_masked_outname = os.path.join(os.path.dirname(InSAR_ts_filename),'velocity_GNSS_integrated_msk.h5');\n",
    "iargs = [corrected_vel_outname, '-m', maskfile, '-o', corrected_vel_masked_outname]\n",
    "import mintpy.cli.mask\n",
    "mintpy.cli.mask.main(iargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a21f1dde-df76-4c84-8591-a4d8b7440589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_gdal failed.\n",
      "cd  C:\\Users\\bvarugu\\Documents\\ARIASenA35\\track39W\\mintpy\n",
      "save_gdal.py velocity_GNSS_integrated_msk.h5 -o ARIASenA35_39N_Asc_velocity_2022_2024_GNSS.tif\n"
     ]
    }
   ],
   "source": [
    "\n",
    "iargs = [corrected_vel_masked_outname, '-o', save_gdal_filename];\n",
    "import mintpy.cli.save_gdal\n",
    "try:\n",
    "    mintpy.cli.save_gdal.main(iargs)\n",
    "except:\n",
    "    print('save_gdal failed.')\n",
    "    print('cd ',os.path.dirname(corrected_vel_masked_outname))\n",
    "    print('save_gdal.py '+os.path.basename(corrected_vel_masked_outname)+' -o '+os.path.basename(save_gdal_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d760766-8d4d-4d19-810a-2fe2ebf2d166",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mintpy",
   "language": "python",
   "name": "mintpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
